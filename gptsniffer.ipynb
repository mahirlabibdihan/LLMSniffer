{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10033521,"sourceType":"datasetVersion","datasetId":6179855},{"sourceId":182503,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":155566,"modelId":178032}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Add dataset \"GPTSniffer\" as input first","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, Trainer, TrainingArguments\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import confusion_matrix\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\" # export OMP_NUM_THREADS=1\nos.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=1 \nos.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=1\nos.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=1\nimport pandas as pd\nimport numpy as np\n#from timm.optim.lion import Lion\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:10.183454Z","iopub.execute_input":"2024-12-03T17:50:10.183802Z","iopub.status.idle":"2024-12-03T17:50:10.189813Z","shell.execute_reply.started":"2024-12-03T17:50:10.183773Z","shell.execute_reply":"2024-12-03T17:50:10.188853Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"import random\n\n# Set seeds for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# If using GPU, set the seed for the CUDA operations as well\ntorch.cuda.manual_seed_all(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:10.191203Z","iopub.execute_input":"2024-12-03T17:50:10.191492Z","iopub.status.idle":"2024-12-03T17:50:10.202691Z","shell.execute_reply.started":"2024-12-03T17:50:10.191450Z","shell.execute_reply":"2024-12-03T17:50:10.202034Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def get_code_without_comments(filepath):\n    with open(filepath, 'rb') as f:\n        lines = f.readlines()\n    code_lines = []\n    for line in tokenize.tokenize(lines.__iter__().__next__):\n        if line.type != tokenize.COMMENT:\n            code_lines.append(line.string)\n    return ''.join(code_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:10.203601Z","iopub.execute_input":"2024-12-03T17:50:10.203901Z","iopub.status.idle":"2024-12-03T17:50:10.214932Z","shell.execute_reply.started":"2024-12-03T17:50:10.203869Z","shell.execute_reply":"2024-12-03T17:50:10.214223Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import re\n\ndef remove_java_comments(filepath):\n    with open(filepath, 'r') as f:\n        code = f.read()  # Read the entire file content\n\n    # Remove single-line comments (//)\n    code = re.sub(r'//.*', '', code)\n\n    # Remove multi-line comments (/* ... */)\n    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n\n    return code\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:10.216865Z","iopub.execute_input":"2024-12-03T17:50:10.217192Z","iopub.status.idle":"2024-12-03T17:50:10.227710Z","shell.execute_reply.started":"2024-12-03T17:50:10.217156Z","shell.execute_reply":"2024-12-03T17:50:10.226891Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# define the dataset\nclass CodeDataset(Dataset):\n    def __init__(self, directory):\n        self.samples = []\n        for filename in os.listdir(directory):\n            label = int(filename.split('_')[0])\n            code = remove_java_comments(os.path.join(directory, filename))\n            self.samples.append((code, label))\n            # with open(os.path.join(directory, filename), 'r') as f:\n            #     code = f.read()\n            #     self.samples.append((code, label))\n    \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        code, label = self.samples[index]\n        inputs = tokenizer.encode_plus(code, padding='max_length', max_length=512, truncation=True)\n        input_ids = inputs['input_ids']\n        attention_mask = inputs['attention_mask']\n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long), \n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long), \n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:10.228636Z","iopub.execute_input":"2024-12-03T17:50:10.228909Z","iopub.status.idle":"2024-12-03T17:50:10.240394Z","shell.execute_reply.started":"2024-12-03T17:50:10.228885Z","shell.execute_reply":"2024-12-03T17:50:10.239576Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive/',force_remount=False)\n#DATA_PATH = '/content/drive/My Drive/Colab Notebooks/SourceSniffer/'\nDATA_PATH = '/kaggle/input/gptsniffer/dataset-1/dataset-1/'\nfrom os.path import join\n\n\n# Set the directory where the training and testing data is stored\ntrain_data_path = join(DATA_PATH,'training_data')\ntest_data_path = join(DATA_PATH,'testing_data') \n\n\n# Define the training dataset and dataloader\ntrain_dataset = CodeDataset(train_data_path)\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# Define the testing dataset and dataloader\ntest_dataset = CodeDataset(test_data_path)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n\n# Set device to GPU if available, otherwise use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#device = torch.device('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:10.325331Z","iopub.execute_input":"2024-12-03T17:50:10.325721Z","iopub.status.idle":"2024-12-03T17:50:12.456877Z","shell.execute_reply.started":"2024-12-03T17:50:10.325689Z","shell.execute_reply":"2024-12-03T17:50:12.455890Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"class SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, features, labels):\n        \"\"\"\n        features: Tensor of shape [batch_size, feature_dim]\n        labels: Tensor of shape [batch_size]\n        \"\"\"\n        batch_size = features.size(0)\n        mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float()  # Binary label mask\n        features_normalized = features / features.norm(dim=1, keepdim=True)  # Normalize features\n        \n        logits = torch.div(\n            torch.mm(features_normalized, features_normalized.T),\n            self.temperature\n        )  # Pairwise similarity logits\n        logits_max = torch.max(logits, dim=1, keepdim=True).values\n        logits -= logits_max.detach()  # Numerical stability\n\n        exp_logits = torch.exp(logits) * mask  # Mask for same-class samples\n        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-12)\n\n        # Compute mean log-probability for positive pairs\n        mask_sum = mask.sum(dim=1)\n        mean_log_prob_pos = (log_prob * mask).sum(dim=1) / (mask_sum + 1e-12)  # Avoid division by zero\n        loss = -mean_log_prob_pos.mean()  # Average loss\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:12.458689Z","iopub.execute_input":"2024-12-03T17:50:12.458968Z","iopub.status.idle":"2024-12-03T17:50:12.465880Z","shell.execute_reply.started":"2024-12-03T17:50:12.458941Z","shell.execute_reply":"2024-12-03T17:50:12.464911Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Model with Binary Classifier\nclass CodeBERTBinaryClassifier(nn.Module):\n    def __init__(self, encoder_model, hidden_size=256, num_layers=2):\n        super(CodeBERTBinaryClassifier, self).__init__()\n        self.encoder = encoder_model\n        # self.classifier = nn.Linear(self.encoder.config.hidden_size, 1)  # Binary classification\n\n        # self.classifier = nn.Sequential(\n        #     nn.Dropout(0.3),  # Dropout with 30%\n        #     nn.Linear(self.encoder.config.hidden_size, 1)\n        # )\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),  # Dropout with 30%\n            nn.Linear(self.encoder.config.hidden_size, 128),  # Hidden layer with 128 units\n            nn.BatchNorm1d(128),  # Batch normalization for the hidden layer\n            nn.ReLU(),  # ReLU activation for the hidden layer\n            nn.Dropout(0.3),  # Dropout with 30%\n            nn.Linear(128, 1)  # Output layer with 1 unit\n        )\n        \n        # # Define an MLP classifier with hidden layers\n        # layers = []\n        # input_size = self.encoder.config.hidden_size  # Output size of the encoder\n        # output_size = 1  # For binary classification\n        \n        # # Adding hidden layers to the MLP\n        # for _ in range(num_layers):\n        #     layers.append(nn.Linear(input_size, hidden_size))\n        #     layers.append(nn.ReLU())  # Use ReLU activation after each hidden layer\n        #     layers.append(nn.Dropout(0.1))  # Optional: dropout for regularization\n        #     input_size = hidden_size  # The output size of the previous layer becomes input size\n        \n        # # Output layer\n        # layers.append(nn.Linear(hidden_size, output_size))\n        # layers.append(nn.Sigmoid())  # Sigmoid for binary classification output\n        \n        # # Combine all layers into a sequential module\n        # self.classifier = nn.Sequential(*layers)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token representation\n        logits = self.classifier(cls_output.detach()).squeeze(-1)  # Squeeze for binary logit\n        return logits, cls_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:12.467068Z","iopub.execute_input":"2024-12-03T17:50:12.467634Z","iopub.status.idle":"2024-12-03T17:50:12.482141Z","shell.execute_reply.started":"2024-12-03T17:50:12.467607Z","shell.execute_reply":"2024-12-03T17:50:12.481415Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Define the tokenizer and the model\n# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n# base_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\nbase_model = AutoModel.from_pretrained(\"microsoft/graphcodebert-base\")\n\nmodel = CodeBERTBinaryClassifier(base_model)\nmodel = model.to(device)\n\n# optimizer = optim.AdamW(model.parameters(), lr=2e-5)\noptimizer = optim.AdamW(\n    [\n        {\"params\": model.encoder.parameters(), \"lr\": 1e-5},  # Pre-trained layers\n        {\"params\": model.classifier.parameters(), \"lr\": 1e-4},     # Task-specific head\n    ],\n    weight_decay=1e-2,\n)\ncriterion = SupConLoss(temperature=0.07)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:12.483380Z","iopub.execute_input":"2024-12-03T17:50:12.483730Z","iopub.status.idle":"2024-12-03T17:50:13.847563Z","shell.execute_reply.started":"2024-12-03T17:50:12.483692Z","shell.execute_reply":"2024-12-03T17:50:13.846606Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)  # Probabilities\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal_loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:13.849359Z","iopub.execute_input":"2024-12-03T17:50:13.849639Z","iopub.status.idle":"2024-12-03T17:50:13.854892Z","shell.execute_reply.started":"2024-12-03T17:50:13.849613Z","shell.execute_reply":"2024-12-03T17:50:13.853970Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn.functional as F\n# Define BCEWithLogitsLoss\nclassification_loss_fn = torch.nn.BCEWithLogitsLoss()\n# Training Loop\nalpha, beta = 2.0, 1.0  # Loss scaling factors\nepochs = 6\n\nalpha = 0.75  # Weight for positive class\ngamma = 2.0   # Focusing parameter\nfocal_loss_fn = FocalLoss(alpha=alpha, gamma=gamma)\n\nmodel.train()\nfor epoch in range(epochs):\n    epoch_loss = 0\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        optimizer.zero_grad()\n        logits, features = model(input_ids, attention_mask)\n\n        features = F.normalize(features, p=2, dim=1)\n        supcon_loss  = criterion(features, labels.float())\n        # BCE loss on logits\n        bce_loss = nn.BCEWithLogitsLoss()(logits.squeeze(-1), labels.float())\n        # Combine the two losses\n        # total_loss = supcon_loss + bce_loss\n        total_loss = alpha * supcon_loss + beta * bce_loss\n        \n        # Backpropagation and optimizer step\n        total_loss.backward()\n        optimizer.step()\n        epoch_loss += total_loss.item()\n    print(f\"Epoch {epoch+1}/{epochs} Loss: {epoch_loss/len(train_dataloader):.4f}\")\n    torch.save(model.state_dict(), f'gpt_classifier_{epoch}.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:50:13.855885Z","iopub.execute_input":"2024-12-03T17:50:13.856212Z","iopub.status.idle":"2024-12-03T17:57:13.203251Z","shell.execute_reply.started":"2024-12-03T17:50:13.856186Z","shell.execute_reply":"2024-12-03T17:57:13.202257Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/6: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/6 Loss: 1.9065\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/6: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/6 Loss: 1.7302\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/6: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/6 Loss: 1.7328\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/6: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/6 Loss: 1.7161\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/6: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/6 Loss: 1.7352\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/6: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/6 Loss: 1.7249\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"torch.save(model.state_dict(), \"codebert_binary_classifier.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:57:13.204570Z","iopub.execute_input":"2024-12-03T17:57:13.204849Z","iopub.status.idle":"2024-12-03T17:57:14.483527Z","shell.execute_reply.started":"2024-12-03T17:57:13.204823Z","shell.execute_reply":"2024-12-03T17:57:14.482528Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef infer_test_dataset(model, test_dataset, batch_size=8, device=\"cuda\"):\n    model.eval()  # Set the model to evaluation mode\n    dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    predictions = []\n    logits_list = []\n\n    with torch.no_grad():  # Disable gradient computation for inference\n        for batch in tqdm(dataloader, desc=\"Inferencing\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n\n            # Forward pass through the model\n            logits, _ = model(input_ids, attention_mask)\n            logits = logits.cpu()  # Move logits back to CPU for processing\n            probabilities = torch.sigmoid(logits)\n            preds = (probabilities > 0.5).long()  # Threshold at 0.5 for binary labels\n\n            predictions.extend(preds.tolist())\n            logits_list.extend(probabilities.tolist())\n\n    return predictions, logits_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:57:14.484823Z","iopub.execute_input":"2024-12-03T17:57:14.485115Z","iopub.status.idle":"2024-12-03T17:57:14.492423Z","shell.execute_reply.started":"2024-12-03T17:57:14.485088Z","shell.execute_reply":"2024-12-03T17:57:14.491233Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# checkpoint = torch.load(\"/kaggle/working/gpt_classifier_5.pth\")\n# model.load_state_dict(checkpoint)\n\n\n# Ground truth labels (from your test dataset)\ntrue_labels = [sample['labels'].item() for sample in test_dataset]\n\npredictions, logits = infer_test_dataset(model, test_dataset, batch_size=8, device=device)\n\n# Analyze predictions\n# for i, (pred, logit, true) in enumerate(zip(predictions, logits, true_labels)):\n#     print(f\"Sample {i}: Predicted Label = {pred}, Logit = {logit}, True Label = {true}\")\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# Calculate metrics\naccuracy = accuracy_score(true_labels, predictions)\nprecision = precision_score(true_labels, predictions)\nrecall = recall_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\nconf_matrix = confusion_matrix(true_labels, predictions)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:57:14.493875Z","iopub.execute_input":"2024-12-03T17:57:14.494369Z","iopub.status.idle":"2024-12-03T17:57:19.867417Z","shell.execute_reply.started":"2024-12-03T17:57:14.494305Z","shell.execute_reply":"2024-12-03T17:57:19.866509Z"}},"outputs":[{"name":"stderr","text":"Inferencing: 100%|██████████| 35/35 [00:04<00:00,  7.41it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.7766\nPrecision: 0.8070\nRecall: 0.7023\nF1 Score: 0.7510\nConfusion Matrix:\n[[120  22]\n [ 39  92]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":56}]}